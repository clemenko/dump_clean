Cluster "ucp" set.
User "admin" set.
Context "admin@ucp" created.
Switched to context "admin@ucp".
Name:           calico-node
Selector:       k8s-app=calico-node
Node-Selector:  <none>
Labels:         k8s-app=calico-node
Annotations:    kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"k8s-app":"calico-node"},"name":"calico-node","namespace":...
Desired Number of Nodes Scheduled: 4
Current Number of Nodes Scheduled: 4
Number of Nodes Scheduled with Up-to-date Pods: 4
Number of Nodes Scheduled with Available Pods: 4
Number of Nodes Misscheduled: 0
Pods Status:  4 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=calico-node
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  cni-plugin
  Containers:
   calico-node:
    Image:  docker/ucp-calico-node:3.0.2
    Port:   <none>
    Requests:
      cpu:      250m
    Liveness:   http-get http://:9099/liveness delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  http-get http://:9099/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      ETCD_ENDPOINTS:                     <set to the key 'etcd_endpoints' of config map 'calico-config'>  Optional: false
      CALICO_NETWORKING_BACKEND:          <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false
      CLUSTER_TYPE:                       k8s,bgp
      CALICO_DISABLE_FILE_LOGGING:        true
      CALICO_IPV4POOL_IPIP:               always
      FELIX_IPINIPENABLED:                true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      CALICO_IPV4POOL_CIDR:               <set to the key 'calico_pool' of config map 'calico-config'>  Optional: false
      CALICO_K8S_NODE_REF:                 (v1:spec.nodeName)
      FELIX_IPV6SUPPORT:                  false
      FELIX_LOGSEVERITYSCREEN:            info
      FELIX_IPINIPMTU:                    <set to the key 'felix_ipinipmtu' of config map 'calico-config'>  Optional: false
      ETCD_CA_CERT_FILE:                  <set to the key 'etcd_ca' of config map 'calico-config'>          Optional: false
      ETCD_KEY_FILE:                      <set to the key 'etcd_key' of config map 'calico-config'>         Optional: false
      ETCD_CERT_FILE:                     <set to the key 'etcd_cert' of config map 'calico-config'>        Optional: false
      NODENAME:                            (v1:spec.nodeName)
      IP:                                  (v1:status.hostIP)
      FELIX_HEALTHENABLED:                true
    Mounts:
      /calico-secrets from etcd-certs (rw)
      /lib/modules from lib-modules (ro)
      /var/run/calico from var-run-calico (rw)
   install-cni:
    Image:  docker/ucp-calico-cni:3.0.2
    Port:   <none>
    Command:
      /install-cni.sh
    Environment:
      CNI_CONF_NAME:       10-calico.conflist
      ETCD_ENDPOINTS:      <set to the key 'etcd_endpoints' of config map 'calico-config'>      Optional: false
      CNI_NETWORK_CONFIG:  <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
    Mounts:
      /calico-secrets from etcd-certs (rw)
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
  Volumes:
   lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
   var-run-calico:
    Type:  HostPath (bare host directory volume)
    Path:  /var/run/calico
   cni-bin-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /opt/cni/bin
   cni-net-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/cni/net.d
   etcd-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-etcd-secrets
    Optional:    false
Events:          <none>


Name:               calico-kube-controllers
Namespace:          kube-system
CreationTimestamp:  Thu, 21 Jun 2018 20:17:30 +0000
Labels:             k8s-app=calico-kube-controllers
Annotations:        deployment.kubernetes.io/revision=2
                    kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"calico-kube-controllers"},"name":"calico-kube-...
Selector:           k8s-app=calico-kube-controllers
Replicas:           1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:       Recreate
MinReadySeconds:    0
Pod Template:
  Labels:           k8s-app=calico-kube-controllers
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  cni-plugin
  Containers:
   calico-kube-controllers:
    Image:  docker/ucp-calico-kube-controllers:3.0.2
    Port:   <none>
    Environment:
      ENABLED_CONTROLLERS:  node,policy,profile,workloadendpoint
      ETCD_ENDPOINTS:       <set to the key 'etcd_endpoints' of config map 'calico-config'>  Optional: false
      ETCD_CA_CERT_FILE:    <set to the key 'etcd_ca' of config map 'calico-config'>         Optional: false
      ETCD_KEY_FILE:        <set to the key 'etcd_key' of config map 'calico-config'>        Optional: false
      ETCD_CERT_FILE:       <set to the key 'etcd_cert' of config map 'calico-config'>       Optional: false
    Mounts:
      /calico-secrets from etcd-certs (rw)
  Volumes:
   etcd-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-etcd-secrets
    Optional:    false
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   calico-kube-controllers-54f4d75698 (1/1 replicas created)
Events:          <none>


Name:                   compose
Namespace:              kube-system
CreationTimestamp:      Thu, 21 Jun 2018 20:17:39 +0000
Labels:                 app=stacks
Annotations:            deployment.kubernetes.io/revision=2
                        kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"name":"compose","namespace":"kube-system"},"spec":{"template":{"me...
Selector:               app=stacks
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
  Labels:           app=stacks
  Service Account:  compose
  Containers:
   ucp-kube-compose:
    Image:  docker/ucp-kube-compose:3.0.2
    Port:   <none>
    Args:
      -kubeconfig
      
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   compose-6df8746695 (1/1 replicas created)
Events:          <none>


Name:                   kube-dns
Namespace:              kube-system
CreationTimestamp:      Thu, 21 Jun 2018 20:17:40 +0000
Labels:                 k8s-app=kube-dns
Annotations:            deployment.kubernetes.io/revision=2
                        kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"kube-dns"},"name":"kube-dns","namespace":"kube...
Selector:               k8s-app=kube-dns
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  0 max unavailable, 10% max surge
Pod Template:
  Labels:           k8s-app=kube-dns
  Service Account:  kube-dns
  Containers:
   ucp-kubedns:
    Image:  docker/ucp-kube-dns:3.0.2
    Ports:  10053/UDP, 10053/TCP, 10055/TCP
    Args:
      --domain=cluster.local.
      --dns-port=10053
      --config-dir=/kube-dns-config
      --v=2
    Limits:
      memory:  170Mi
    Requests:
      cpu:      100m
      memory:   70Mi
    Liveness:   http-get http://:10054/healthcheck/kubedns delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:  http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3
    Environment:
      PROMETHEUS_PORT:  10055
    Mounts:
      /kube-dns-config from kube-dns-config (rw)
   ucp-dnsmasq-nanny:
    Image:  docker/ucp-kube-dns-dnsmasq-nanny:3.0.2
    Ports:  53/UDP, 53/TCP
    Args:
      -v=2
      -logtostderr
      -configDir=/etc/k8s/dns/dnsmasq-nanny
      -restartDnsmasq=true
      --
      -k
      --cache-size=1000
      --log-facility=-
      --server=/cluster.local/127.0.0.1#10053
      --server=/in-addr.arpa/127.0.0.1#10053
      --server=/ip6.arpa/127.0.0.1#10053
    Requests:
      cpu:        150m
      memory:     20Mi
    Liveness:     http-get http://:10054/healthcheck/dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:
      /etc/k8s/dns/dnsmasq-nanny from kube-dns-config (rw)
   ucp-kubedns-sidecar:
    Image:  docker/ucp-kube-dns-sidecar:3.0.2
    Port:   10054/TCP
    Args:
      --v=2
      --logtostderr
      --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
      --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
    Requests:
      cpu:        10m
      memory:     20Mi
    Liveness:     http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:       <none>
  Volumes:
   kube-dns-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-dns
    Optional:  true
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   kube-dns-fbf6ff8fb (1/1 replicas created)
Events:          <none>


Name:           calico-kube-controllers-54f4d75698
Namespace:      kube-system
Selector:       k8s-app=calico-kube-controllers,pod-template-hash=1090831254
Labels:         k8s-app=calico-kube-controllers
                pod-template-hash=1090831254
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=1
                deployment.kubernetes.io/revision=2
Controlled By:  Deployment/calico-kube-controllers
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=calico-kube-controllers
                    pod-template-hash=1090831254
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  cni-plugin
  Containers:
   calico-kube-controllers:
    Image:  docker/ucp-calico-kube-controllers:3.0.2
    Port:   <none>
    Environment:
      ENABLED_CONTROLLERS:  node,policy,profile,workloadendpoint
      ETCD_ENDPOINTS:       <set to the key 'etcd_endpoints' of config map 'calico-config'>  Optional: false
      ETCD_CA_CERT_FILE:    <set to the key 'etcd_ca' of config map 'calico-config'>         Optional: false
      ETCD_KEY_FILE:        <set to the key 'etcd_key' of config map 'calico-config'>        Optional: false
      ETCD_CERT_FILE:       <set to the key 'etcd_cert' of config map 'calico-config'>       Optional: false
    Mounts:
      /calico-secrets from etcd-certs (rw)
  Volumes:
   etcd-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-etcd-secrets
    Optional:    false
Events:          <none>


Name:           calico-kube-controllers-64ffdc6dbf
Namespace:      kube-system
Selector:       k8s-app=calico-kube-controllers,pod-template-hash=2099872869
Labels:         k8s-app=calico-kube-controllers
                pod-template-hash=2099872869
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=1
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/calico-kube-controllers
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=calico-kube-controllers
                    pod-template-hash=2099872869
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  cni-plugin
  Containers:
   calico-kube-controllers:
    Image:  docker/ucp-calico-kube-controllers:3.0.1
    Port:   <none>
    Environment:
      ENABLED_CONTROLLERS:  node,policy,profile,workloadendpoint
      ETCD_ENDPOINTS:       <set to the key 'etcd_endpoints' of config map 'calico-config'>  Optional: false
      ETCD_CA_CERT_FILE:    <set to the key 'etcd_ca' of config map 'calico-config'>         Optional: false
      ETCD_KEY_FILE:        <set to the key 'etcd_key' of config map 'calico-config'>        Optional: false
      ETCD_CERT_FILE:       <set to the key 'etcd_cert' of config map 'calico-config'>       Optional: false
    Mounts:
      /calico-secrets from etcd-certs (rw)
  Volumes:
   etcd-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-etcd-secrets
    Optional:    false
Events:          <none>


Name:           compose-6df8746695
Namespace:      kube-system
Selector:       app=stacks,pod-template-hash=2894302251
Labels:         app=stacks
                pod-template-hash=2894302251
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=2
Controlled By:  Deployment/compose
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=stacks
                    pod-template-hash=2894302251
  Service Account:  compose
  Containers:
   ucp-kube-compose:
    Image:  docker/ucp-kube-compose:3.0.2
    Port:   <none>
    Args:
      -kubeconfig
      
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:           <none>


Name:           compose-76b854f656
Namespace:      kube-system
Selector:       app=stacks,pod-template-hash=3264109212
Labels:         app=stacks
                pod-template-hash=3264109212
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/compose
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=stacks
                    pod-template-hash=3264109212
  Service Account:  compose
  Containers:
   ucp-kube-compose:
    Image:  docker/ucp-kube-compose:3.0.1
    Port:   <none>
    Args:
      -kubeconfig
      
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:           <none>


Name:           kube-dns-6d48bc9b79
Namespace:      kube-system
Selector:       k8s-app=kube-dns,pod-template-hash=2804675635
Labels:         k8s-app=kube-dns
                pod-template-hash=2804675635
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/kube-dns
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-dns
                    pod-template-hash=2804675635
  Service Account:  kube-dns
  Containers:
   ucp-kubedns:
    Image:  docker/ucp-kube-dns:3.0.1
    Ports:  10053/UDP, 10053/TCP, 10055/TCP
    Args:
      --domain=cluster.local.
      --dns-port=10053
      --config-dir=/kube-dns-config
      --v=2
    Limits:
      memory:  170Mi
    Requests:
      cpu:      100m
      memory:   70Mi
    Liveness:   http-get http://:10054/healthcheck/kubedns delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:  http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3
    Environment:
      PROMETHEUS_PORT:  10055
    Mounts:
      /kube-dns-config from kube-dns-config (rw)
   ucp-dnsmasq-nanny:
    Image:  docker/ucp-kube-dns-dnsmasq-nanny:3.0.1
    Ports:  53/UDP, 53/TCP
    Args:
      -v=2
      -logtostderr
      -configDir=/etc/k8s/dns/dnsmasq-nanny
      -restartDnsmasq=true
      --
      -k
      --cache-size=1000
      --log-facility=-
      --server=/cluster.local/127.0.0.1#10053
      --server=/in-addr.arpa/127.0.0.1#10053
      --server=/ip6.arpa/127.0.0.1#10053
    Requests:
      cpu:        150m
      memory:     20Mi
    Liveness:     http-get http://:10054/healthcheck/dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:
      /etc/k8s/dns/dnsmasq-nanny from kube-dns-config (rw)
   ucp-kubedns-sidecar:
    Image:  docker/ucp-kube-dns-sidecar:3.0.1
    Port:   10054/TCP
    Args:
      --v=2
      --logtostderr
      --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
      --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
    Requests:
      cpu:        10m
      memory:     20Mi
    Liveness:     http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:       <none>
  Volumes:
   kube-dns-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-dns
    Optional:  true
Events:        <none>


Name:           kube-dns-fbf6ff8fb
Namespace:      kube-system
Selector:       k8s-app=kube-dns,pod-template-hash=969299496
Labels:         k8s-app=kube-dns
                pod-template-hash=969299496
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=2
Controlled By:  Deployment/kube-dns
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-dns
                    pod-template-hash=969299496
  Service Account:  kube-dns
  Containers:
   ucp-kubedns:
    Image:  docker/ucp-kube-dns:3.0.2
    Ports:  10053/UDP, 10053/TCP, 10055/TCP
    Args:
      --domain=cluster.local.
      --dns-port=10053
      --config-dir=/kube-dns-config
      --v=2
    Limits:
      memory:  170Mi
    Requests:
      cpu:      100m
      memory:   70Mi
    Liveness:   http-get http://:10054/healthcheck/kubedns delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:  http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3
    Environment:
      PROMETHEUS_PORT:  10055
    Mounts:
      /kube-dns-config from kube-dns-config (rw)
   ucp-dnsmasq-nanny:
    Image:  docker/ucp-kube-dns-dnsmasq-nanny:3.0.2
    Ports:  53/UDP, 53/TCP
    Args:
      -v=2
      -logtostderr
      -configDir=/etc/k8s/dns/dnsmasq-nanny
      -restartDnsmasq=true
      --
      -k
      --cache-size=1000
      --log-facility=-
      --server=/cluster.local/127.0.0.1#10053
      --server=/in-addr.arpa/127.0.0.1#10053
      --server=/ip6.arpa/127.0.0.1#10053
    Requests:
      cpu:        150m
      memory:     20Mi
    Liveness:     http-get http://:10054/healthcheck/dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:
      /etc/k8s/dns/dnsmasq-nanny from kube-dns-config (rw)
   ucp-kubedns-sidecar:
    Image:  docker/ucp-kube-dns-sidecar:3.0.2
    Port:   10054/TCP
    Args:
      --v=2
      --logtostderr
      --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
      --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
    Requests:
      cpu:        10m
      memory:     20Mi
    Liveness:     http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:       <none>
  Volumes:
   kube-dns-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-dns
    Optional:  true
Events:        <none>


Name:               calico-kube-controllers
Namespace:          kube-system
CreationTimestamp:  Thu, 21 Jun 2018 20:17:30 +0000
Labels:             k8s-app=calico-kube-controllers
Annotations:        deployment.kubernetes.io/revision=2
                    kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"calico-kube-controllers"},"name":"calico-kube-...
Selector:           k8s-app=calico-kube-controllers
Replicas:           1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:       Recreate
MinReadySeconds:    0
Pod Template:
  Labels:           k8s-app=calico-kube-controllers
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  cni-plugin
  Containers:
   calico-kube-controllers:
    Image:  docker/ucp-calico-kube-controllers:3.0.2
    Port:   <none>
    Environment:
      ENABLED_CONTROLLERS:  node,policy,profile,workloadendpoint
      ETCD_ENDPOINTS:       <set to the key 'etcd_endpoints' of config map 'calico-config'>  Optional: false
      ETCD_CA_CERT_FILE:    <set to the key 'etcd_ca' of config map 'calico-config'>         Optional: false
      ETCD_KEY_FILE:        <set to the key 'etcd_key' of config map 'calico-config'>        Optional: false
      ETCD_CERT_FILE:       <set to the key 'etcd_cert' of config map 'calico-config'>       Optional: false
    Mounts:
      /calico-secrets from etcd-certs (rw)
  Volumes:
   etcd-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-etcd-secrets
    Optional:    false
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   calico-kube-controllers-54f4d75698 (1/1 replicas created)
Events:          <none>


Name:                   compose
Namespace:              kube-system
CreationTimestamp:      Thu, 21 Jun 2018 20:17:39 +0000
Labels:                 app=stacks
Annotations:            deployment.kubernetes.io/revision=2
                        kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"name":"compose","namespace":"kube-system"},"spec":{"template":{"me...
Selector:               app=stacks
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
  Labels:           app=stacks
  Service Account:  compose
  Containers:
   ucp-kube-compose:
    Image:  docker/ucp-kube-compose:3.0.2
    Port:   <none>
    Args:
      -kubeconfig
      
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   compose-6df8746695 (1/1 replicas created)
Events:          <none>


Name:                   kube-dns
Namespace:              kube-system
CreationTimestamp:      Thu, 21 Jun 2018 20:17:40 +0000
Labels:                 k8s-app=kube-dns
Annotations:            deployment.kubernetes.io/revision=2
                        kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"kube-dns"},"name":"kube-dns","namespace":"kube...
Selector:               k8s-app=kube-dns
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  0 max unavailable, 10% max surge
Pod Template:
  Labels:           k8s-app=kube-dns
  Service Account:  kube-dns
  Containers:
   ucp-kubedns:
    Image:  docker/ucp-kube-dns:3.0.2
    Ports:  10053/UDP, 10053/TCP, 10055/TCP
    Args:
      --domain=cluster.local.
      --dns-port=10053
      --config-dir=/kube-dns-config
      --v=2
    Limits:
      memory:  170Mi
    Requests:
      cpu:      100m
      memory:   70Mi
    Liveness:   http-get http://:10054/healthcheck/kubedns delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:  http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3
    Environment:
      PROMETHEUS_PORT:  10055
    Mounts:
      /kube-dns-config from kube-dns-config (rw)
   ucp-dnsmasq-nanny:
    Image:  docker/ucp-kube-dns-dnsmasq-nanny:3.0.2
    Ports:  53/UDP, 53/TCP
    Args:
      -v=2
      -logtostderr
      -configDir=/etc/k8s/dns/dnsmasq-nanny
      -restartDnsmasq=true
      --
      -k
      --cache-size=1000
      --log-facility=-
      --server=/cluster.local/127.0.0.1#10053
      --server=/in-addr.arpa/127.0.0.1#10053
      --server=/ip6.arpa/127.0.0.1#10053
    Requests:
      cpu:        150m
      memory:     20Mi
    Liveness:     http-get http://:10054/healthcheck/dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:
      /etc/k8s/dns/dnsmasq-nanny from kube-dns-config (rw)
   ucp-kubedns-sidecar:
    Image:  docker/ucp-kube-dns-sidecar:3.0.2
    Port:   10054/TCP
    Args:
      --v=2
      --logtostderr
      --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
      --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
    Requests:
      cpu:        10m
      memory:     20Mi
    Liveness:     http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:       <none>
  Volumes:
   kube-dns-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-dns
    Optional:  true
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   kube-dns-fbf6ff8fb (1/1 replicas created)
Events:          <none>


Name:           calico-node
Selector:       k8s-app=calico-node
Node-Selector:  <none>
Labels:         k8s-app=calico-node
Annotations:    kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"k8s-app":"calico-node"},"name":"calico-node","namespace":...
Desired Number of Nodes Scheduled: 4
Current Number of Nodes Scheduled: 4
Number of Nodes Scheduled with Up-to-date Pods: 4
Number of Nodes Scheduled with Available Pods: 4
Number of Nodes Misscheduled: 0
Pods Status:  4 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=calico-node
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  cni-plugin
  Containers:
   calico-node:
    Image:  docker/ucp-calico-node:3.0.2
    Port:   <none>
    Requests:
      cpu:      250m
    Liveness:   http-get http://:9099/liveness delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  http-get http://:9099/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      ETCD_ENDPOINTS:                     <set to the key 'etcd_endpoints' of config map 'calico-config'>  Optional: false
      CALICO_NETWORKING_BACKEND:          <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false
      CLUSTER_TYPE:                       k8s,bgp
      CALICO_DISABLE_FILE_LOGGING:        true
      CALICO_IPV4POOL_IPIP:               always
      FELIX_IPINIPENABLED:                true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      CALICO_IPV4POOL_CIDR:               <set to the key 'calico_pool' of config map 'calico-config'>  Optional: false
      CALICO_K8S_NODE_REF:                 (v1:spec.nodeName)
      FELIX_IPV6SUPPORT:                  false
      FELIX_LOGSEVERITYSCREEN:            info
      FELIX_IPINIPMTU:                    <set to the key 'felix_ipinipmtu' of config map 'calico-config'>  Optional: false
      ETCD_CA_CERT_FILE:                  <set to the key 'etcd_ca' of config map 'calico-config'>          Optional: false
      ETCD_KEY_FILE:                      <set to the key 'etcd_key' of config map 'calico-config'>         Optional: false
      ETCD_CERT_FILE:                     <set to the key 'etcd_cert' of config map 'calico-config'>        Optional: false
      NODENAME:                            (v1:spec.nodeName)
      IP:                                  (v1:status.hostIP)
      FELIX_HEALTHENABLED:                true
    Mounts:
      /calico-secrets from etcd-certs (rw)
      /lib/modules from lib-modules (ro)
      /var/run/calico from var-run-calico (rw)
   install-cni:
    Image:  docker/ucp-calico-cni:3.0.2
    Port:   <none>
    Command:
      /install-cni.sh
    Environment:
      CNI_CONF_NAME:       10-calico.conflist
      ETCD_ENDPOINTS:      <set to the key 'etcd_endpoints' of config map 'calico-config'>      Optional: false
      CNI_NETWORK_CONFIG:  <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
    Mounts:
      /calico-secrets from etcd-certs (rw)
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
  Volumes:
   lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
   var-run-calico:
    Type:  HostPath (bare host directory volume)
    Path:  /var/run/calico
   cni-bin-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /opt/cni/bin
   cni-net-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/cni/net.d
   etcd-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-etcd-secrets
    Optional:    false
Events:          <none>


Name:           calico-kube-controllers-54f4d75698
Namespace:      kube-system
Selector:       k8s-app=calico-kube-controllers,pod-template-hash=1090831254
Labels:         k8s-app=calico-kube-controllers
                pod-template-hash=1090831254
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=1
                deployment.kubernetes.io/revision=2
Controlled By:  Deployment/calico-kube-controllers
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=calico-kube-controllers
                    pod-template-hash=1090831254
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  cni-plugin
  Containers:
   calico-kube-controllers:
    Image:  docker/ucp-calico-kube-controllers:3.0.2
    Port:   <none>
    Environment:
      ENABLED_CONTROLLERS:  node,policy,profile,workloadendpoint
      ETCD_ENDPOINTS:       <set to the key 'etcd_endpoints' of config map 'calico-config'>  Optional: false
      ETCD_CA_CERT_FILE:    <set to the key 'etcd_ca' of config map 'calico-config'>         Optional: false
      ETCD_KEY_FILE:        <set to the key 'etcd_key' of config map 'calico-config'>        Optional: false
      ETCD_CERT_FILE:       <set to the key 'etcd_cert' of config map 'calico-config'>       Optional: false
    Mounts:
      /calico-secrets from etcd-certs (rw)
  Volumes:
   etcd-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-etcd-secrets
    Optional:    false
Events:          <none>


Name:           calico-kube-controllers-64ffdc6dbf
Namespace:      kube-system
Selector:       k8s-app=calico-kube-controllers,pod-template-hash=2099872869
Labels:         k8s-app=calico-kube-controllers
                pod-template-hash=2099872869
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=1
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/calico-kube-controllers
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=calico-kube-controllers
                    pod-template-hash=2099872869
  Annotations:      scheduler.alpha.kubernetes.io/critical-pod=
  Service Account:  cni-plugin
  Containers:
   calico-kube-controllers:
    Image:  docker/ucp-calico-kube-controllers:3.0.1
    Port:   <none>
    Environment:
      ENABLED_CONTROLLERS:  node,policy,profile,workloadendpoint
      ETCD_ENDPOINTS:       <set to the key 'etcd_endpoints' of config map 'calico-config'>  Optional: false
      ETCD_CA_CERT_FILE:    <set to the key 'etcd_ca' of config map 'calico-config'>         Optional: false
      ETCD_KEY_FILE:        <set to the key 'etcd_key' of config map 'calico-config'>        Optional: false
      ETCD_CERT_FILE:       <set to the key 'etcd_cert' of config map 'calico-config'>       Optional: false
    Mounts:
      /calico-secrets from etcd-certs (rw)
  Volumes:
   etcd-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-etcd-secrets
    Optional:    false
Events:          <none>


Name:           compose-6df8746695
Namespace:      kube-system
Selector:       app=stacks,pod-template-hash=2894302251
Labels:         app=stacks
                pod-template-hash=2894302251
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=2
Controlled By:  Deployment/compose
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=stacks
                    pod-template-hash=2894302251
  Service Account:  compose
  Containers:
   ucp-kube-compose:
    Image:  docker/ucp-kube-compose:3.0.2
    Port:   <none>
    Args:
      -kubeconfig
      
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:           <none>


Name:           compose-76b854f656
Namespace:      kube-system
Selector:       app=stacks,pod-template-hash=3264109212
Labels:         app=stacks
                pod-template-hash=3264109212
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/compose
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=stacks
                    pod-template-hash=3264109212
  Service Account:  compose
  Containers:
   ucp-kube-compose:
    Image:  docker/ucp-kube-compose:3.0.1
    Port:   <none>
    Args:
      -kubeconfig
      
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:           <none>


Name:           kube-dns-6d48bc9b79
Namespace:      kube-system
Selector:       k8s-app=kube-dns,pod-template-hash=2804675635
Labels:         k8s-app=kube-dns
                pod-template-hash=2804675635
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/kube-dns
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-dns
                    pod-template-hash=2804675635
  Service Account:  kube-dns
  Containers:
   ucp-kubedns:
    Image:  docker/ucp-kube-dns:3.0.1
    Ports:  10053/UDP, 10053/TCP, 10055/TCP
    Args:
      --domain=cluster.local.
      --dns-port=10053
      --config-dir=/kube-dns-config
      --v=2
    Limits:
      memory:  170Mi
    Requests:
      cpu:      100m
      memory:   70Mi
    Liveness:   http-get http://:10054/healthcheck/kubedns delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:  http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3
    Environment:
      PROMETHEUS_PORT:  10055
    Mounts:
      /kube-dns-config from kube-dns-config (rw)
   ucp-dnsmasq-nanny:
    Image:  docker/ucp-kube-dns-dnsmasq-nanny:3.0.1
    Ports:  53/UDP, 53/TCP
    Args:
      -v=2
      -logtostderr
      -configDir=/etc/k8s/dns/dnsmasq-nanny
      -restartDnsmasq=true
      --
      -k
      --cache-size=1000
      --log-facility=-
      --server=/cluster.local/127.0.0.1#10053
      --server=/in-addr.arpa/127.0.0.1#10053
      --server=/ip6.arpa/127.0.0.1#10053
    Requests:
      cpu:        150m
      memory:     20Mi
    Liveness:     http-get http://:10054/healthcheck/dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:
      /etc/k8s/dns/dnsmasq-nanny from kube-dns-config (rw)
   ucp-kubedns-sidecar:
    Image:  docker/ucp-kube-dns-sidecar:3.0.1
    Port:   10054/TCP
    Args:
      --v=2
      --logtostderr
      --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
      --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
    Requests:
      cpu:        10m
      memory:     20Mi
    Liveness:     http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:       <none>
  Volumes:
   kube-dns-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-dns
    Optional:  true
Events:        <none>


Name:           kube-dns-fbf6ff8fb
Namespace:      kube-system
Selector:       k8s-app=kube-dns,pod-template-hash=969299496
Labels:         k8s-app=kube-dns
                pod-template-hash=969299496
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=2
Controlled By:  Deployment/kube-dns
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-dns
                    pod-template-hash=969299496
  Service Account:  kube-dns
  Containers:
   ucp-kubedns:
    Image:  docker/ucp-kube-dns:3.0.2
    Ports:  10053/UDP, 10053/TCP, 10055/TCP
    Args:
      --domain=cluster.local.
      --dns-port=10053
      --config-dir=/kube-dns-config
      --v=2
    Limits:
      memory:  170Mi
    Requests:
      cpu:      100m
      memory:   70Mi
    Liveness:   http-get http://:10054/healthcheck/kubedns delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:  http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3
    Environment:
      PROMETHEUS_PORT:  10055
    Mounts:
      /kube-dns-config from kube-dns-config (rw)
   ucp-dnsmasq-nanny:
    Image:  docker/ucp-kube-dns-dnsmasq-nanny:3.0.2
    Ports:  53/UDP, 53/TCP
    Args:
      -v=2
      -logtostderr
      -configDir=/etc/k8s/dns/dnsmasq-nanny
      -restartDnsmasq=true
      --
      -k
      --cache-size=1000
      --log-facility=-
      --server=/cluster.local/127.0.0.1#10053
      --server=/in-addr.arpa/127.0.0.1#10053
      --server=/ip6.arpa/127.0.0.1#10053
    Requests:
      cpu:        150m
      memory:     20Mi
    Liveness:     http-get http://:10054/healthcheck/dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:
      /etc/k8s/dns/dnsmasq-nanny from kube-dns-config (rw)
   ucp-kubedns-sidecar:
    Image:  docker/ucp-kube-dns-sidecar:3.0.2
    Port:   10054/TCP
    Args:
      --v=2
      --logtostderr
      --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
      --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
    Requests:
      cpu:        10m
      memory:     20Mi
    Liveness:     http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:       <none>
  Volumes:
   kube-dns-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-dns
    Optional:  true
Events:        <none>


Name:           calico-kube-controllers-54f4d75698-mxpdh
Namespace:      kube-system
Node:           ip-172-31-9-58.ec2.internal/172.31.9.58
Start Time:     Thu, 21 Jun 2018 20:58:42 +0000
Labels:         k8s-app=calico-kube-controllers
                pod-template-hash=1090831254
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"kube-system","name":"calico-kube-controllers-54f4d75698","uid":"e1691587-7595-11e...
                scheduler.alpha.kubernetes.io/critical-pod=
Status:         Running
IP:             172.31.9.58
Created By:     ReplicaSet/calico-kube-controllers-54f4d75698
Controlled By:  ReplicaSet/calico-kube-controllers-54f4d75698
Containers:
  calico-kube-controllers:
    Container ID:   docker://be99c8be69fc43225d29417d175dde38f81b4d781fbf2d4e0ff2ca3b2dfb5753
    Image:          docker/ucp-calico-kube-controllers:3.0.2
    Image ID:       docker-pullable://docker/ucp-calico-kube-controllers@sha256:83e15ea08801a4bbf3f2693d7ca7fcdf77c996c9af9300a8160450604af331d6
    Port:           <none>
    State:          Running
      Started:      Thu, 28 Jun 2018 16:57:26 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    2
      Started:      Thu, 28 Jun 2018 16:43:34 +0000
      Finished:     Thu, 28 Jun 2018 16:56:31 +0000
    Ready:          True
    Restart Count:  2
    Environment:
      ENABLED_CONTROLLERS:  node,policy,profile,workloadendpoint
      ETCD_ENDPOINTS:       <set to the key 'etcd_endpoints' of config map 'calico-config'>  Optional: false
      ETCD_CA_CERT_FILE:    <set to the key 'etcd_ca' of config map 'calico-config'>         Optional: false
      ETCD_KEY_FILE:        <set to the key 'etcd_key' of config map 'calico-config'>        Optional: false
      ETCD_CERT_FILE:       <set to the key 'etcd_cert' of config map 'calico-config'>       Optional: false
    Mounts:
      /calico-secrets from etcd-certs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from cni-plugin-token-8rk2q (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  etcd-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-etcd-secrets
    Optional:    false
  cni-plugin-token-8rk2q:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cni-plugin-token-8rk2q
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     CriticalAddonsOnly
                 com.docker.ucp.manager
                 com.docker.ucp.orchestrator.kubernetes
                 node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
                 node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
Events:          <none>


Name:           calico-node-ftttl
Namespace:      kube-system
Node:           ip-172-31-12-152.ec2.internal/172.31.12.152
Start Time:     Thu, 21 Jun 2018 20:59:01 +0000
Labels:         controller-revision-hash=2479745248
                k8s-app=calico-node
                pod-template-generation=2
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"calico-node","uid":"1f830495-7590-11e8-8955-0242ac110012","ap...
                scheduler.alpha.kubernetes.io/critical-pod=
Status:         Running
IP:             172.31.12.152
Created By:     DaemonSet/calico-node
Controlled By:  DaemonSet/calico-node
Containers:
  calico-node:
    Container ID:   docker://2d0cd4f25f6a9257fa59844be860470c32c87b019bfec02ca25e32b6b9c1e622
    Image:          docker/ucp-calico-node:3.0.2
    Image ID:       docker-pullable://docker/ucp-calico-node@sha256:ab0f31de5e6675795ae048bf0917ae91687ace35f94b61d381bf4b18bb7c610b
    Port:           <none>
    State:          Running
      Started:      Thu, 28 Jun 2018 17:01:11 +0000
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 21 Jun 2018 20:59:02 +0000
      Finished:     Thu, 28 Jun 2018 17:00:15 +0000
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:      250m
    Liveness:   http-get http://:9099/liveness delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  http-get http://:9099/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      ETCD_ENDPOINTS:                     <set to the key 'etcd_endpoints' of config map 'calico-config'>  Optional: false
      CALICO_NETWORKING_BACKEND:          <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false
      CLUSTER_TYPE:                       k8s,bgp
      CALICO_DISABLE_FILE_LOGGING:        true
      CALICO_IPV4POOL_IPIP:               always
      FELIX_IPINIPENABLED:                true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      CALICO_IPV4POOL_CIDR:               <set to the key 'calico_pool' of config map 'calico-config'>  Optional: false
      CALICO_K8S_NODE_REF:                 (v1:spec.nodeName)
      FELIX_IPV6SUPPORT:                  false
      FELIX_LOGSEVERITYSCREEN:            info
      FELIX_IPINIPMTU:                    <set to the key 'felix_ipinipmtu' of config map 'calico-config'>  Optional: false
      ETCD_CA_CERT_FILE:                  <set to the key 'etcd_ca' of config map 'calico-config'>          Optional: false
      ETCD_KEY_FILE:                      <set to the key 'etcd_key' of config map 'calico-config'>         Optional: false
      ETCD_CERT_FILE:                     <set to the key 'etcd_cert' of config map 'calico-config'>        Optional: false
      NODENAME:                            (v1:spec.nodeName)
      IP:                                  (v1:status.hostIP)
      FELIX_HEALTHENABLED:                true
    Mounts:
      /calico-secrets from etcd-certs (rw)
      /lib/modules from lib-modules (ro)
      /var/run/calico from var-run-calico (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from cni-plugin-token-8rk2q (ro)
  install-cni:
    Container ID:  docker://a9eb2d37a556be4eed856d428fd8af6dce43c764fc7c9949060413031e669b01
    Image:         docker/ucp-calico-cni:3.0.2
    Image ID:      docker-pullable://docker/ucp-calico-cni@sha256:d19d15e27d12292037a863c2104479b0cca419727a4360baada26767022ed375
    Port:          <none>
    Command:
      /install-cni.sh
    State:          Running
      Started:      Thu, 28 Jun 2018 17:01:12 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    137
      Started:      Thu, 21 Jun 2018 20:59:03 +0000
      Finished:     Thu, 28 Jun 2018 17:00:25 +0000
    Ready:          True
    Restart Count:  1
    Environment:
      CNI_CONF_NAME:       10-calico.conflist
      ETCD_ENDPOINTS:      <set to the key 'etcd_endpoints' of config map 'calico-config'>      Optional: false
      CNI_NETWORK_CONFIG:  <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
    Mounts:
      /calico-secrets from etcd-certs (rw)
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from cni-plugin-token-8rk2q (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
  var-run-calico:
    Type:  HostPath (bare host directory volume)
    Path:  /var/run/calico
  cni-bin-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /opt/cni/bin
  cni-net-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/cni/net.d
  etcd-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-etcd-secrets
    Optional:    false
  cni-plugin-token-8rk2q:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cni-plugin-token-8rk2q
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     CriticalAddonsOnly
                 com.docker.ucp.manager
                 com.docker.ucp.orchestrator.kubernetes
                 node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           calico-node-qqn9t
Namespace:      kube-system
Node:           ip-172-31-10-96.ec2.internal/172.31.10.96
Start Time:     Thu, 21 Jun 2018 20:58:53 +0000
Labels:         controller-revision-hash=2479745248
                k8s-app=calico-node
                pod-template-generation=2
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"calico-node","uid":"1f830495-7590-11e8-8955-0242ac110012","ap...
                scheduler.alpha.kubernetes.io/critical-pod=
Status:         Running
IP:             172.31.10.96
Created By:     DaemonSet/calico-node
Controlled By:  DaemonSet/calico-node
Containers:
  calico-node:
    Container ID:   docker://1499d10d651fc92071273d1ed7b4fb228ea7a57d07806f2ced0566837753b459
    Image:          docker/ucp-calico-node:3.0.2
    Image ID:       docker-pullable://docker/ucp-calico-node@sha256:ab0f31de5e6675795ae048bf0917ae91687ace35f94b61d381bf4b18bb7c610b
    Port:           <none>
    State:          Running
      Started:      Thu, 28 Jun 2018 16:50:16 +0000
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 21 Jun 2018 20:58:54 +0000
      Finished:     Thu, 28 Jun 2018 16:49:53 +0000
    Ready:          True
    Restart Count:  1
    Requests:
      cpu:      250m
    Liveness:   http-get http://:9099/liveness delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  http-get http://:9099/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      ETCD_ENDPOINTS:                     <set to the key 'etcd_endpoints' of config map 'calico-config'>  Optional: false
      CALICO_NETWORKING_BACKEND:          <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false
      CLUSTER_TYPE:                       k8s,bgp
      CALICO_DISABLE_FILE_LOGGING:        true
      CALICO_IPV4POOL_IPIP:               always
      FELIX_IPINIPENABLED:                true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      CALICO_IPV4POOL_CIDR:               <set to the key 'calico_pool' of config map 'calico-config'>  Optional: false
      CALICO_K8S_NODE_REF:                 (v1:spec.nodeName)
      FELIX_IPV6SUPPORT:                  false
      FELIX_LOGSEVERITYSCREEN:            info
      FELIX_IPINIPMTU:                    <set to the key 'felix_ipinipmtu' of config map 'calico-config'>  Optional: false
      ETCD_CA_CERT_FILE:                  <set to the key 'etcd_ca' of config map 'calico-config'>          Optional: false
      ETCD_KEY_FILE:                      <set to the key 'etcd_key' of config map 'calico-config'>         Optional: false
      ETCD_CERT_FILE:                     <set to the key 'etcd_cert' of config map 'calico-config'>        Optional: false
      NODENAME:                            (v1:spec.nodeName)
      IP:                                  (v1:status.hostIP)
      FELIX_HEALTHENABLED:                true
    Mounts:
      /calico-secrets from etcd-certs (rw)
      /lib/modules from lib-modules (ro)
      /var/run/calico from var-run-calico (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from cni-plugin-token-8rk2q (ro)
  install-cni:
    Container ID:  docker://3b70771b6aeb18187f5df72128b047555c7a3b2d33980925404e7413c378b730
    Image:         docker/ucp-calico-cni:3.0.2
    Image ID:      docker-pullable://docker/ucp-calico-cni@sha256:d19d15e27d12292037a863c2104479b0cca419727a4360baada26767022ed375
    Port:          <none>
    Command:
      /install-cni.sh
    State:          Running
      Started:      Thu, 28 Jun 2018 16:50:16 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    137
      Started:      Thu, 21 Jun 2018 20:58:54 +0000
      Finished:     Thu, 28 Jun 2018 16:50:03 +0000
    Ready:          True
    Restart Count:  1
    Environment:
      CNI_CONF_NAME:       10-calico.conflist
      ETCD_ENDPOINTS:      <set to the key 'etcd_endpoints' of config map 'calico-config'>      Optional: false
      CNI_NETWORK_CONFIG:  <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
    Mounts:
      /calico-secrets from etcd-certs (rw)
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from cni-plugin-token-8rk2q (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
  var-run-calico:
    Type:  HostPath (bare host directory volume)
    Path:  /var/run/calico
  cni-bin-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /opt/cni/bin
  cni-net-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/cni/net.d
  etcd-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-etcd-secrets
    Optional:    false
  cni-plugin-token-8rk2q:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cni-plugin-token-8rk2q
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     CriticalAddonsOnly
                 com.docker.ucp.manager
                 com.docker.ucp.orchestrator.kubernetes
                 node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           calico-node-v74wg
Namespace:      kube-system
Node:           ip-172-31-9-58.ec2.internal/172.31.9.58
Start Time:     Thu, 21 Jun 2018 20:58:40 +0000
Labels:         controller-revision-hash=2479745248
                k8s-app=calico-node
                pod-template-generation=2
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"calico-node","uid":"1f830495-7590-11e8-8955-0242ac110012","ap...
                scheduler.alpha.kubernetes.io/critical-pod=
Status:         Running
IP:             172.31.9.58
Created By:     DaemonSet/calico-node
Controlled By:  DaemonSet/calico-node
Containers:
  calico-node:
    Container ID:   docker://e80e2497ab9aed6f5d9f23a555b6b8164690319b941cc620df60bff0e0521b7d
    Image:          docker/ucp-calico-node:3.0.2
    Image ID:       docker-pullable://docker/ucp-calico-node@sha256:ab0f31de5e6675795ae048bf0917ae91687ace35f94b61d381bf4b18bb7c610b
    Port:           <none>
    State:          Running
      Started:      Thu, 28 Jun 2018 16:57:26 +0000
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 28 Jun 2018 16:43:36 +0000
      Finished:     Thu, 28 Jun 2018 16:56:31 +0000
    Ready:          True
    Restart Count:  2
    Requests:
      cpu:      250m
    Liveness:   http-get http://:9099/liveness delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  http-get http://:9099/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      ETCD_ENDPOINTS:                     <set to the key 'etcd_endpoints' of config map 'calico-config'>  Optional: false
      CALICO_NETWORKING_BACKEND:          <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false
      CLUSTER_TYPE:                       k8s,bgp
      CALICO_DISABLE_FILE_LOGGING:        true
      CALICO_IPV4POOL_IPIP:               always
      FELIX_IPINIPENABLED:                true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      CALICO_IPV4POOL_CIDR:               <set to the key 'calico_pool' of config map 'calico-config'>  Optional: false
      CALICO_K8S_NODE_REF:                 (v1:spec.nodeName)
      FELIX_IPV6SUPPORT:                  false
      FELIX_LOGSEVERITYSCREEN:            info
      FELIX_IPINIPMTU:                    <set to the key 'felix_ipinipmtu' of config map 'calico-config'>  Optional: false
      ETCD_CA_CERT_FILE:                  <set to the key 'etcd_ca' of config map 'calico-config'>          Optional: false
      ETCD_KEY_FILE:                      <set to the key 'etcd_key' of config map 'calico-config'>         Optional: false
      ETCD_CERT_FILE:                     <set to the key 'etcd_cert' of config map 'calico-config'>        Optional: false
      NODENAME:                            (v1:spec.nodeName)
      IP:                                  (v1:status.hostIP)
      FELIX_HEALTHENABLED:                true
    Mounts:
      /calico-secrets from etcd-certs (rw)
      /lib/modules from lib-modules (ro)
      /var/run/calico from var-run-calico (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from cni-plugin-token-8rk2q (ro)
  install-cni:
    Container ID:  docker://8393517ca5ab7b696f1aa64f27548f4b2725cc8547ec8644dfa6e710c0546d42
    Image:         docker/ucp-calico-cni:3.0.2
    Image ID:      docker-pullable://docker/ucp-calico-cni@sha256:d19d15e27d12292037a863c2104479b0cca419727a4360baada26767022ed375
    Port:          <none>
    Command:
      /install-cni.sh
    State:          Running
      Started:      Thu, 28 Jun 2018 16:57:27 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    137
      Started:      Thu, 28 Jun 2018 16:43:38 +0000
      Finished:     Thu, 28 Jun 2018 16:56:41 +0000
    Ready:          True
    Restart Count:  2
    Environment:
      CNI_CONF_NAME:       10-calico.conflist
      ETCD_ENDPOINTS:      <set to the key 'etcd_endpoints' of config map 'calico-config'>      Optional: false
      CNI_NETWORK_CONFIG:  <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
    Mounts:
      /calico-secrets from etcd-certs (rw)
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from cni-plugin-token-8rk2q (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
  var-run-calico:
    Type:  HostPath (bare host directory volume)
    Path:  /var/run/calico
  cni-bin-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /opt/cni/bin
  cni-net-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/cni/net.d
  etcd-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-etcd-secrets
    Optional:    false
  cni-plugin-token-8rk2q:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cni-plugin-token-8rk2q
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     CriticalAddonsOnly
                 com.docker.ucp.manager
                 com.docker.ucp.orchestrator.kubernetes
                 node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           calico-node-xj54s
Namespace:      kube-system
Node:           ip-172-31-18-229.ec2.internal/172.31.18.229
Start Time:     Thu, 21 Jun 2018 20:58:35 +0000
Labels:         controller-revision-hash=2479745248
                k8s-app=calico-node
                pod-template-generation=2
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"DaemonSet","namespace":"kube-system","name":"calico-node","uid":"1f830495-7590-11e8-8955-0242ac110012","ap...
                scheduler.alpha.kubernetes.io/critical-pod=
Status:         Running
IP:             172.31.18.229
Created By:     DaemonSet/calico-node
Controlled By:  DaemonSet/calico-node
Containers:
  calico-node:
    Container ID:   docker://f4e647a860480b3fa4da2d43f4b3b11d1e9db565c68ffd6cdd4ab633b9f33c53
    Image:          docker/ucp-calico-node:3.0.2
    Image ID:       docker-pullable://docker/ucp-calico-node@sha256:ab0f31de5e6675795ae048bf0917ae91687ace35f94b61d381bf4b18bb7c610b
    Port:           <none>
    State:          Running
      Started:      Thu, 28 Jun 2018 17:27:20 +0000
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 28 Jun 2018 17:03:50 +0000
      Finished:     Thu, 28 Jun 2018 17:26:57 +0000
    Ready:          True
    Restart Count:  2
    Requests:
      cpu:      250m
    Liveness:   http-get http://:9099/liveness delay=10s timeout=1s period=10s #success=1 #failure=6
    Readiness:  http-get http://:9099/readiness delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      ETCD_ENDPOINTS:                     <set to the key 'etcd_endpoints' of config map 'calico-config'>  Optional: false
      CALICO_NETWORKING_BACKEND:          <set to the key 'calico_backend' of config map 'calico-config'>  Optional: false
      CLUSTER_TYPE:                       k8s,bgp
      CALICO_DISABLE_FILE_LOGGING:        true
      CALICO_IPV4POOL_IPIP:               always
      FELIX_IPINIPENABLED:                true
      FELIX_DEFAULTENDPOINTTOHOSTACTION:  ACCEPT
      CALICO_IPV4POOL_CIDR:               <set to the key 'calico_pool' of config map 'calico-config'>  Optional: false
      CALICO_K8S_NODE_REF:                 (v1:spec.nodeName)
      FELIX_IPV6SUPPORT:                  false
      FELIX_LOGSEVERITYSCREEN:            info
      FELIX_IPINIPMTU:                    <set to the key 'felix_ipinipmtu' of config map 'calico-config'>  Optional: false
      ETCD_CA_CERT_FILE:                  <set to the key 'etcd_ca' of config map 'calico-config'>          Optional: false
      ETCD_KEY_FILE:                      <set to the key 'etcd_key' of config map 'calico-config'>         Optional: false
      ETCD_CERT_FILE:                     <set to the key 'etcd_cert' of config map 'calico-config'>        Optional: false
      NODENAME:                            (v1:spec.nodeName)
      IP:                                  (v1:status.hostIP)
      FELIX_HEALTHENABLED:                true
    Mounts:
      /calico-secrets from etcd-certs (rw)
      /lib/modules from lib-modules (ro)
      /var/run/calico from var-run-calico (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from cni-plugin-token-8rk2q (ro)
  install-cni:
    Container ID:  docker://5712e1123828461d707c5cf08fbe6571e3a577766bf540779b02da7751c9ed4d
    Image:         docker/ucp-calico-cni:3.0.2
    Image ID:      docker-pullable://docker/ucp-calico-cni@sha256:d19d15e27d12292037a863c2104479b0cca419727a4360baada26767022ed375
    Port:          <none>
    Command:
      /install-cni.sh
    State:          Running
      Started:      Thu, 28 Jun 2018 17:27:20 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    137
      Started:      Thu, 28 Jun 2018 17:03:51 +0000
      Finished:     Thu, 28 Jun 2018 17:27:07 +0000
    Ready:          True
    Restart Count:  2
    Environment:
      CNI_CONF_NAME:       10-calico.conflist
      ETCD_ENDPOINTS:      <set to the key 'etcd_endpoints' of config map 'calico-config'>      Optional: false
      CNI_NETWORK_CONFIG:  <set to the key 'cni_network_config' of config map 'calico-config'>  Optional: false
    Mounts:
      /calico-secrets from etcd-certs (rw)
      /host/etc/cni/net.d from cni-net-dir (rw)
      /host/opt/cni/bin from cni-bin-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from cni-plugin-token-8rk2q (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  lib-modules:
    Type:  HostPath (bare host directory volume)
    Path:  /lib/modules
  var-run-calico:
    Type:  HostPath (bare host directory volume)
    Path:  /var/run/calico
  cni-bin-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /opt/cni/bin
  cni-net-dir:
    Type:  HostPath (bare host directory volume)
    Path:  /etc/cni/net.d
  etcd-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  calico-etcd-secrets
    Optional:    false
  cni-plugin-token-8rk2q:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cni-plugin-token-8rk2q
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     CriticalAddonsOnly
                 com.docker.ucp.manager
                 com.docker.ucp.orchestrator.kubernetes
                 node-role.kubernetes.io/master:NoSchedule
                 node.alpha.kubernetes.io/notReady:NoExecute
                 node.alpha.kubernetes.io/unreachable:NoExecute
                 node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:          <none>


Name:           compose-6df8746695-d6q2m
Namespace:      kube-system
Node:           ip-172-31-9-58.ec2.internal/172.31.9.58
Start Time:     Thu, 21 Jun 2018 20:58:31 +0000
Labels:         app=stacks
                pod-template-hash=2894302251
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"kube-system","name":"compose-6df8746695","uid":"dacb5a5c-7595-11e8-b33c-0242ac110...
Status:         Running
IP:             192.168.171.7
Created By:     ReplicaSet/compose-6df8746695
Controlled By:  ReplicaSet/compose-6df8746695
Containers:
  ucp-kube-compose:
    Container ID:  docker://5bf4d394111dc1155edeb35ff56d9d1a7e8a1704c475981f1d2805752ee1f147
    Image:         docker/ucp-kube-compose:3.0.2
    Image ID:      docker-pullable://docker/ucp-kube-compose@sha256:f4dc1550e7d678af3d799ea4e85e38447fbd0c206a0e1fbbb91a706c4a7c6b4e
    Port:          <none>
    Args:
      -kubeconfig
      
    State:          Running
      Started:      Thu, 28 Jun 2018 16:57:26 +0000
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 28 Jun 2018 16:43:39 +0000
      Finished:     Thu, 28 Jun 2018 16:56:31 +0000
    Ready:          True
    Restart Count:  2
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from compose-token-c7pr7 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  compose-token-c7pr7:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  compose-token-c7pr7
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     com.docker.ucp.manager
                 com.docker.ucp.orchestrator.kubernetes
                 node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:           kube-dns-fbf6ff8fb-w5zw8
Namespace:      kube-system
Node:           ip-172-31-9-58.ec2.internal/172.31.9.58
Start Time:     Thu, 21 Jun 2018 20:58:33 +0000
Labels:         k8s-app=kube-dns
                pod-template-hash=969299496
Annotations:    kubernetes.io/created-by={"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicaSet","namespace":"kube-system","name":"kube-dns-fbf6ff8fb","uid":"dba5b81c-7595-11e8-b33c-0242ac110...
Status:         Running
IP:             192.168.171.8
Created By:     ReplicaSet/kube-dns-fbf6ff8fb
Controlled By:  ReplicaSet/kube-dns-fbf6ff8fb
Containers:
  ucp-kubedns:
    Container ID:  docker://0a67fae0775fe23f61dbcdca9d1a212a8a91c165b40805316bd832a5f4087950
    Image:         docker/ucp-kube-dns:3.0.2
    Image ID:      docker-pullable://docker/ucp-kube-dns@sha256:72a3dd5a581ebb01b85427424e6edc9ca22e9c7363c409fed41709a77cba97e2
    Ports:         10053/UDP, 10053/TCP, 10055/TCP
    Args:
      --domain=cluster.local.
      --dns-port=10053
      --config-dir=/kube-dns-config
      --v=2
    State:          Running
      Started:      Thu, 28 Jun 2018 16:57:27 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    137
      Started:      Thu, 28 Jun 2018 16:43:38 +0000
      Finished:     Thu, 28 Jun 2018 16:56:41 +0000
    Ready:          True
    Restart Count:  2
    Limits:
      memory:  170Mi
    Requests:
      cpu:      100m
      memory:   70Mi
    Liveness:   http-get http://:10054/healthcheck/kubedns delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:  http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3
    Environment:
      PROMETHEUS_PORT:  10055
    Mounts:
      /kube-dns-config from kube-dns-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-5xm27 (ro)
  ucp-dnsmasq-nanny:
    Container ID:  docker://59b09110ad4f6d6aa659e76fa9283a11f2d01e99f0001c14c1daa3a3c7852c10
    Image:         docker/ucp-kube-dns-dnsmasq-nanny:3.0.2
    Image ID:      docker-pullable://docker/ucp-kube-dns-dnsmasq-nanny@sha256:b0f5f5f73e884e77b0077d5d47cc9e1a0b55b3db177484b5da09e7aa353d71e3
    Ports:         53/UDP, 53/TCP
    Args:
      -v=2
      -logtostderr
      -configDir=/etc/k8s/dns/dnsmasq-nanny
      -restartDnsmasq=true
      --
      -k
      --cache-size=1000
      --log-facility=-
      --server=/cluster.local/127.0.0.1#10053
      --server=/in-addr.arpa/127.0.0.1#10053
      --server=/ip6.arpa/127.0.0.1#10053
    State:          Running
      Started:      Thu, 28 Jun 2018 16:57:27 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    137
      Started:      Thu, 28 Jun 2018 16:43:38 +0000
      Finished:     Thu, 28 Jun 2018 16:56:41 +0000
    Ready:          True
    Restart Count:  2
    Requests:
      cpu:        150m
      memory:     20Mi
    Liveness:     http-get http://:10054/healthcheck/dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:
      /etc/k8s/dns/dnsmasq-nanny from kube-dns-config (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-5xm27 (ro)
  ucp-kubedns-sidecar:
    Container ID:  docker://06d6840f1c167e0c05a3ed8c50831955dcea4f851ccccd095ccf71e0ca903e64
    Image:         docker/ucp-kube-dns-sidecar:3.0.2
    Image ID:      docker-pullable://docker/ucp-kube-dns-sidecar@sha256:fed12eab8a45aa94dac28df8891dc9a8fe6071dd014f492ce56e0da21391ec47
    Port:          10054/TCP
    Args:
      --v=2
      --logtostderr
      --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
      --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
    State:          Running
      Started:      Thu, 28 Jun 2018 16:57:27 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    2
      Started:      Thu, 28 Jun 2018 16:43:39 +0000
      Finished:     Thu, 28 Jun 2018 16:56:31 +0000
    Ready:          True
    Restart Count:  2
    Requests:
      cpu:        10m
      memory:     20Mi
    Liveness:     http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-5xm27 (ro)
Conditions:
  Type           Status
  Initialized    True 
  Ready          True 
  PodScheduled   True 
Volumes:
  kube-dns-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-dns
    Optional:  true
  kube-dns-token-5xm27:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-dns-token-5xm27
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/master=
Tolerations:     CriticalAddonsOnly
                 com.docker.ucp.manager
                 com.docker.ucp.orchestrator.kubernetes
                 node.alpha.kubernetes.io/notReady:NoExecute for 300s
                 node.alpha.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:              kube-dns
Namespace:         kube-system
Labels:            k8s-app=kube-dns
                   kubernetes.io/cluster-service=true
                   kubernetes.io/name=KubeDNS
Annotations:       kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"kube-dns","kubernetes.io/cluster-service":"true","kubernetes.io/n...
Selector:          k8s-app=kube-dns
Type:              ClusterIP
IP:                10.96.0.10
Port:              dns  53/UDP
TargetPort:        53/UDP
Endpoints:         192.168.171.8:53
Port:              dns-tcp  53/TCP
TargetPort:        53/TCP
Endpoints:         192.168.171.8:53
Session Affinity:  None
Events:            <none>
